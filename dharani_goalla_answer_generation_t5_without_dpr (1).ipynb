{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoHR4WsMfGuo",
        "outputId": "2304d0cc-3f01-4ea0-c290-8f046094cfc5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('ms_marco', 'v1.1')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFiInoT4fJAb",
        "outputId": "14fc41d2-d8f7-498b-9b46-099b7ecd6f7f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uivp8vVdgzSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
        "\n",
        "# Set up T5 for question answering using a text-to-text format\n",
        "def answer_question_t5(question, context, max_length=200):\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # Generate the answer tokens\n",
        "    outputs = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFTBa7VVzjPU",
        "outputId": "534d8144-2e65-4d47-8338-1d9fd9745045"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNL0ws_lBglk",
        "outputId": "a3b16cc3-5ecd-47e7-9732-3aa75e06bb12"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWgp7NWdFiaQ",
        "outputId": "c258260b-477a-4830-a797-76470ec52534"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DPRQuestionEncoder, DPRContextEncoder\n",
        "from transformers import DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer"
      ],
      "metadata": {
        "id": "tNTfct6zFyWI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getEncoders(base_path = \"/content/drive/My Drive/Colab Notebooks/DPR/\"):\n",
        "  question_encoder = DPRQuestionEncoder.from_pretrained(f\"{base_path}question_encoder\")\n",
        "  context_encoder = DPRContextEncoder.from_pretrained(f\"{base_path}context_encoder\")\n",
        "  return question_encoder, context_encoder"
      ],
      "metadata": {
        "id": "if5FX1KWFp4j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getTokenizers(base_path = \"/content/drive/My Drive/Colab Notebooks/DPR/\"):\n",
        "  question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(f\"{base_path}question_encoder_tokenizer\")\n",
        "  context_encoder_tokenizer = DPRContextEncoderTokenizer.from_pretrained(f\"{base_path}context_encoder_tokenizer\")\n",
        "  return question_encoder_tokenizer, context_encoder_tokenizer"
      ],
      "metadata": {
        "id": "3UiuGUDQF3MI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_embeddings(embeddings):\n",
        "    norms = embeddings.norm(p=2, dim=1, keepdim=True)\n",
        "    return embeddings.div(norms)\n",
        "\n",
        "def encode_query(query, question_encoder, tokenizer):\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(question_encoder.device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        query_embedding = question_encoder(**inputs).pooler_output\n",
        "    # Normalize the embedding\n",
        "    query_embedding = normalize_embeddings(query_embedding)\n",
        "    return query_embedding\n",
        "\n",
        "def encode_passages(passages, context_encoder, tokenizer):\n",
        "    inputs = tokenizer(passages, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(context_encoder.device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        passage_embeddings = context_encoder(**inputs).pooler_output\n",
        "    # Normalize the embeddings\n",
        "    passage_embeddings = normalize_embeddings(passage_embeddings)\n",
        "    return passage_embeddings"
      ],
      "metadata": {
        "id": "gz-vRB8qGH1-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "question_encoder, context_encoder = getEncoders()\n",
        "question_encoder_tokenizer, context_encoder_tokenizer = getTokenizers()\n",
        "\n",
        "def retrieve_passages(query_embedding, passage_embeddings):\n",
        "    query_embedding = F.normalize(query_embedding, p=2, dim=1)\n",
        "    passage_embeddings = F.normalize(passage_embeddings, p=2, dim=1)\n",
        "    similarities = torch.matmul(query_embedding, passage_embeddings.T)\n",
        "\n",
        "    # Convert similarities to probabilities to rank passages\n",
        "    similarities = similarities.squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Rank passages based on similarities\n",
        "    ranked_indices = similarities.argsort()[::-1]\n",
        "\n",
        "    return ranked_indices\n",
        "\n",
        "\n",
        "def retrieve_top_n_passages(query, passages, n=5):\n",
        "    query_embedding = encode_query(query, question_encoder, question_encoder_tokenizer)\n",
        "    passage_embeddings = encode_passages(passages, context_encoder, context_encoder_tokenizer)\n",
        "    ranked_indices = retrieve_passages(query_embedding, passage_embeddings)\n",
        "    return [passages[idx] for idx in ranked_indices[:n]]\n"
      ],
      "metadata": {
        "id": "HmVhMd5rJHFy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Rouge\n",
        "from rouge import Rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmoLiT1pO-3s",
        "outputId": "3da81f80-dc96-4136-9a06-1c0f48403a75"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Rouge) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edswas6yuSiz",
        "outputId": "4ff7f121-c1d2-408d-c4d4-0ff420b4d9be"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "nltk.download('punkt')\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def calculate_f1(generated_answer, actual_answers):\n",
        "    \"\"\"Calculates the F1 score.\n",
        "\n",
        "    Args:\n",
        "        generated_answer (str): The answer generated by the model.\n",
        "        actual_answers (list[str]): A list of correct answers.\n",
        "\n",
        "    Returns:\n",
        "        float: The F1 score.\n",
        "    \"\"\"\n",
        "    if not actual_answers or not actual_answers[0]:\n",
        "        actual_answers.append(\" \")\n",
        "    generated_tokens = nltk.word_tokenize(generated_answer)\n",
        "    actual_answer_tokens = nltk.word_tokenize(actual_answers[0])\n",
        "\n",
        "    common_tokens = set(generated_tokens) & set(actual_answer_tokens)\n",
        "\n",
        "    precision = len(common_tokens) / len(generated_tokens) if len(generated_tokens) > 0 else 0\n",
        "    recall = len(common_tokens) / len(actual_answer_tokens) if len(actual_answer_tokens) > 0 else 0\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        return 0\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def process_entry(data_entry, useDpr=True):\n",
        "    question = data_entry['query']\n",
        "    context = ', '.join(retrieve_top_n_passages(question, data_entry['passages']['passage_text'], n=3)) if useDpr else ', '.join(data_entry['passages']['passage_text'])\n",
        "    generated_answer = answer_question_t5(question, context)\n",
        "    actual_answers = data_entry['answers']\n",
        "    f1_score = calculate_f1(generated_answer, actual_answers)\n",
        "    rouge = Rouge()\n",
        "    rouge_score = rouge.get_scores(generated_answer, actual_answers[0])[0]['rouge-l']['f']\n",
        "    return (f1_score, rouge_score)\n",
        "\n",
        "def calculate_avg_f1(dataset, useDpr=True,  max_workers=os.cpu_count()):\n",
        "    scores = {\"f1_score\": 0.0, \"rouge_score\": 0.0}\n",
        "    f1_scores = []\n",
        "    rouge_scores = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        future_to_entry = {executor.submit(process_entry, entry, useDpr): entry for entry in dataset}\n",
        "        for future in as_completed(future_to_entry):\n",
        "            f1_score, rouge_score = future.result()\n",
        "            f1_scores.append(f1_score)\n",
        "            rouge_scores.append(rouge_score)\n",
        "\n",
        "    scores[\"f1_score\"] = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
        "    scores[\"rouge_score\"] = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n",
        "    return scores\n",
        "\n",
        "# Usage\n",
        "val_data = dataset['validation']\n",
        "filtered_dataset = [entry for entry in val_data if entry['answers'] and entry['answers'][0]]\n",
        "result = calculate_avg_f1(filtered_dataset, False)\n",
        "print('score', result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vviKC5iAAqDP",
        "outputId": "9930a0b2-e03f-4f5b-afd6-ff1784cbf69c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score {'f1_score': 0.2219083806806626, 'rouge_score': 0.2241029025437013}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pjfVW3mpHP55"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
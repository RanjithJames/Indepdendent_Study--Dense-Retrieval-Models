{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM9EvJihGR8f",
        "outputId": "2a84eeec-d561-4bd1-b78c-8ffb0e0e5be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yuBNMIooGXID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85276c53-888b-4024-adfd-e903fd60d48f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ms_marco\", \"v1.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXFfiD_bHcJd",
        "outputId": "69f0bb56-2f7d-4a12-fae9-fea6f24c4004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    validation: Dataset({\n",
            "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
            "        num_rows: 10047\n",
            "    })\n",
            "    train: Dataset({\n",
            "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
            "        num_rows: 82326\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
            "        num_rows: 9650\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZN8Hmfd4j7D",
        "outputId": "f8073f8f-6dd4-41ea-841f-86c315e55113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CaNInAWaJ7WM"
      },
      "outputs": [],
      "source": [
        "from transformers import DPRQuestionEncoder, DPRContextEncoder\n",
        "from transformers import DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
        "\n",
        "# question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "# context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "\n",
        "# question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "# context_encoder_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "\n",
        "base_path = \"/content/drive/My Drive/Colab Notebooks/DPR/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "luRHQaZA6ue9"
      },
      "outputs": [],
      "source": [
        "def saveModels(question_encoder, context_encoder, base_path = \"/content/drive/My Drive/Colab Notebooks/DPR/\"):\n",
        "  question_encoder.save_pretrained(f\"{base_path}question_encoder\")\n",
        "  context_encoder.save_pretrained(f\"{base_path}context_encoder\")\n",
        "\n",
        "# saveModels(question_encoder, context_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R0AXm63JBfD-"
      },
      "outputs": [],
      "source": [
        "def saveTokenizers(question_encoder_tokenizer, context_encoder_tokenizer, base_path = \"/content/drive/My Drive/Colab Notebooks/DPR/\"):\n",
        "  question_encoder_tokenizer.save_pretrained(f\"{base_path}question_encoder_tokenizer\")\n",
        "  context_encoder_tokenizer.save_pretrained(f\"{base_path}context_encoder_tokenizer\")\n",
        "# saveTokenizers(question_encoder_tokenizer, context_encoder_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y4JLkkZU6W9e"
      },
      "outputs": [],
      "source": [
        "def getEncoders(base_path = \"/content/drive/My Drive/Colab Notebooks/DPR/\"):\n",
        "  question_encoder = DPRQuestionEncoder.from_pretrained(f\"{base_path}question_encoder\")\n",
        "  context_encoder = DPRContextEncoder.from_pretrained(f\"{base_path}context_encoder\")\n",
        "  return question_encoder, context_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dlgNMMr6CVor"
      },
      "outputs": [],
      "source": [
        "def getTokenizers(base_path = \"/content/drive/My Drive/Colab Notebooks/DPR/\"):\n",
        "  question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(f\"{base_path}question_encoder_tokenizer\")\n",
        "  context_encoder_tokenizer = DPRContextEncoderTokenizer.from_pretrained(f\"{base_path}context_encoder_tokenizer\")\n",
        "  return question_encoder_tokenizer, context_encoder_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "def get_paraphrase_response(input_text,num_return_sequences,num_beams):\n",
        "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
        "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  return tgt_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaQGXEpWfbFR",
        "outputId": "0b2d0789-a083-40dd-d2c1-27bde389cab3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering\n",
        "import random\n",
        "\n",
        "\n",
        "def generate_qa_pairs(passages):\n",
        "    # Initialize tokenizer and model for question generation\n",
        "    qg_tokenizer = AutoTokenizer.from_pretrained('mrm8488/t5-base-finetuned-question-generation-ap')\n",
        "    qg_model = AutoModelForSeq2SeqLM.from_pretrained('mrm8488/t5-base-finetuned-question-generation-ap')\n",
        "\n",
        "    # Initialize tokenizer and model for question answering\n",
        "    qa_tokenizer = AutoTokenizer.from_pretrained('deepset/roberta-base-squad2')\n",
        "    qa_model = AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for passage in passages:\n",
        "        # Generate a question from the passage\n",
        "        input_text = \"generate question: \" + passage\n",
        "        inputs = qg_tokenizer.encode_plus(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
        "        outputs = qg_model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=64,\n",
        "            num_beams=5,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        question = qg_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Prepare the model input for answering the question using the passage\n",
        "        answer_input = f\"context: {passage} question: {question}\"\n",
        "        answer_inputs = qa_tokenizer(answer_input, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
        "        answer_outputs = qa_model(**answer_inputs)\n",
        "        start_positions = torch.argmax(answer_outputs.start_logits)\n",
        "        end_positions = torch.argmax(answer_outputs.end_logits) + 1\n",
        "        answer = qa_tokenizer.decode(answer_inputs['input_ids'][0][start_positions:end_positions])\n",
        "\n",
        "        results.append({\n",
        "            'passage': passage,\n",
        "            'question': question,\n",
        "            'answer': answer\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_random_passage(passages):\n",
        "    # Select a random index from the array of passages\n",
        "    index = random.randint(0, len(passages) - 1)\n",
        "    # Retrieve the passage at that index\n",
        "    passage = passages[index]\n",
        "    return index, passage\n",
        "\n",
        "def createNewDataset(datasetToModify):\n",
        "  data_to_generate = [dict(item) for item in datasetToModify]\n",
        "  for item in data_to_generate:\n",
        "    passages = item['passages']['passage_text']\n",
        "    is_selected = [0] * len(passages)\n",
        "\n",
        "    index, selected_passage = get_random_passage(passages)\n",
        "    is_selected[index] = 1\n",
        "    qa_pair = generate_qa_pairs([selected_passage])\n",
        "\n",
        "    item['passages']['is_selected'] = is_selected\n",
        "    item['query'] = qa_pair[0]['question']\n",
        "    item['generated_answer'] = qa_pair[0]['answer']\n",
        "\n",
        "  return data_to_generate\n"
      ],
      "metadata": {
        "id": "Cd-bgJO0x-vM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def preprocess_data(data):\n",
        "    dpr_data = []\n",
        "    for item in data['train']:\n",
        "        query = item['query']\n",
        "        passages = item['passages']['passage_text']\n",
        "        is_selected = item['passages']['is_selected']\n",
        "\n",
        "        for passage, selected in zip(passages, is_selected):\n",
        "            if selected:  # This passage is positive\n",
        "                dpr_data.append({'query': query, 'passage': get_paraphrase_response(passage,1,10)[0], 'label': 1})\n",
        "            else:  # This passage is negative\n",
        "                dpr_data.append({'query': query, 'passage': get_paraphrase_response(passage,1,10)[0], 'label': 0})\n",
        "    return dpr_data"
      ],
      "metadata": {
        "id": "7SZm0uv3f4Zv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def get_response_for_passage(passage, selected):\n",
        "    \"\"\"Function to handle the get_response call for a single passage.\"\"\"\n",
        "    try:\n",
        "        # Assuming get_response returns a list and you want the first element\n",
        "        response = get_paraphrase_response(passage, 1, 10)[0]\n",
        "        return {'passage': response, 'label': int(selected)}\n",
        "    except Exception as exc:\n",
        "        print(f'Error processing passage {passage}: {exc}')\n",
        "        return None\n",
        "\n",
        "def process_item(item):\n",
        "    \"\"\"Function to process a single item with concurrent passage processing.\"\"\"\n",
        "    query = item['query']\n",
        "    passages = item['passages']['passage_text']\n",
        "    is_selected = item['passages']['is_selected']\n",
        "    item_data = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=100) as passage_executor:\n",
        "        # Create a future for each passage\n",
        "        passage_futures = [passage_executor.submit(get_response_for_passage, passage, selected)\n",
        "                           for passage, selected in zip(passages, is_selected)]\n",
        "\n",
        "        # As each future completes, append its result to item_data\n",
        "        for future in as_completed(passage_futures):\n",
        "            result = future.result()\n",
        "            if result:  # Check to make sure an error didn't occur\n",
        "                item_data.append({'query': query, **result})\n",
        "\n",
        "    return item_data\n",
        "\n",
        "def preprocess_for_dpr_concurrently(data):\n",
        "    \"\"\"Concurrent preprocessing for each item and its passages.\"\"\"\n",
        "    dpr_data = []\n",
        "    with ThreadPoolExecutor(max_workers=100) as item_executor:\n",
        "        # Submit all items for processing\n",
        "        item_futures = [item_executor.submit(process_item, item) for item in data['train']]\n",
        "\n",
        "        # Wait for all futures to complete and collect their results\n",
        "        for future in as_completed(item_futures):\n",
        "            try:\n",
        "                item_results = future.result()\n",
        "                dpr_data.extend(item_results)\n",
        "            except Exception as exc:\n",
        "                print(f'An item failed to process: {exc}')\n",
        "                # Handle or log the error as appropriate\n",
        "\n",
        "    return dpr_data\n"
      ],
      "metadata": {
        "id": "68WPZRCQjhG3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(preprocess_for_dpr_concurrent({'train':dataset['train'].select(range(10))}))"
      ],
      "metadata": {
        "id": "jo2Wdq6mkSxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_beams = 10\n",
        "num_return_sequences = 1\n",
        "context = \"The ultimate test of your knowledge is your capacity to convey it to another.\"\n",
        "print(get_paraphrase_response(context,num_return_sequences,num_beams))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh1oG9e5fnwQ",
        "outputId": "9c172d25-4faf-4f00-e558-9204bbf1337f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The test of your knowledge is your ability to convey it.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kodY2bbqL9sx"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "class DPRDataset(Dataset):\n",
        "    def __init__(self, queries, passages, labels, query_tokenizer, context_tokenizer, max_length=256):\n",
        "        self.queries = queries\n",
        "        self.passages = passages\n",
        "        self.labels = labels\n",
        "        self.query_tokenizer = query_tokenizer\n",
        "        self.context_tokenizer = context_tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        query_encodings = self.query_tokenizer(self.queries[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
        "        passage_encodings = self.context_tokenizer(self.passages[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
        "        return {\n",
        "            'query_input_ids': query_encodings['input_ids'].squeeze(0),\n",
        "            'query_attention_mask': query_encodings['attention_mask'].squeeze(0),\n",
        "            'passage_input_ids': passage_encodings['input_ids'].squeeze(0),\n",
        "            'passage_attention_mask': passage_encodings['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "def getDataLoader(data, question_encoder_tokenizer, context_encoder_tokenizer):\n",
        "  queries = [item['query'] for item in preprocessed_data]\n",
        "  passages = [item['passage'] for item in preprocessed_data]\n",
        "  labels = [item['label'] for item in preprocessed_data]\n",
        "\n",
        "  DPRdataset = DPRDataset(queries, passages, labels, question_encoder_tokenizer, context_encoder_tokenizer)\n",
        "  DPRdataloader = DataLoader(DPRdataset, batch_size=16, shuffle=True)\n",
        "  return DPRdataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rBJIbL74K1HY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import DPRQuestionEncoder, DPRContextEncoder\n",
        "from transformers import DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
        "\n",
        "\n",
        "def trainModel(data,  question_encoder, context_encoder,  question_encoder_tokenizer, context_encoder_tokenizer, num_epochs=1):\n",
        "  DPRdataloader = getDataLoader(data, question_encoder_tokenizer, context_encoder_tokenizer)\n",
        "  optimizer_qe = AdamW(question_encoder.parameters(), lr=1e-6)\n",
        "  optimizer_ce = AdamW(context_encoder.parameters(), lr=1e-6)\n",
        "\n",
        "  # Scheduler (optional)\n",
        "  total_steps = len(DPRdataloader) * num_epochs\n",
        "  scheduler_qe = get_linear_schedule_with_warmup(optimizer_qe, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "  scheduler_ce = get_linear_schedule_with_warmup(optimizer_ce, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "  # Loss function\n",
        "  cosine_similarity = nn.CosineSimilarity(dim=1)\n",
        "  margin_loss = nn.MarginRankingLoss(margin=0.01)\n",
        "\n",
        "  # Move models to GPU if available\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  question_encoder.to(device)\n",
        "  context_encoder.to(device)\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "      question_encoder.train()\n",
        "      context_encoder.train()\n",
        "      total_loss = 0\n",
        "      for batch in DPRdataloader:\n",
        "          optimizer_qe.zero_grad()\n",
        "          optimizer_ce.zero_grad()\n",
        "\n",
        "          # Move batch to device\n",
        "          query_input_ids = batch['query_input_ids'].to(device)\n",
        "          query_attention_mask = batch['query_attention_mask'].to(device)\n",
        "          passage_input_ids = batch['passage_input_ids'].to(device)\n",
        "          passage_attention_mask = batch['passage_attention_mask'].to(device)\n",
        "          labels = batch['labels'].to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          query_embeddings = question_encoder(query_input_ids, attention_mask=query_attention_mask).pooler_output\n",
        "          passage_embeddings = context_encoder(passage_input_ids, attention_mask=passage_attention_mask).pooler_output\n",
        "\n",
        "          # Calculate similarities and loss\n",
        "          similarities = cosine_similarity(query_embeddings, passage_embeddings)\n",
        "          target = torch.where(labels > 0, 1, -1).float()  # Convert labels to {-1, 1} for MarginRankingLoss\n",
        "          loss = margin_loss(similarities, torch.zeros_like(similarities), target)\n",
        "\n",
        "          # Backward and optimize\n",
        "          loss.backward()\n",
        "          optimizer_qe.step()\n",
        "          optimizer_ce.step()\n",
        "          scheduler_qe.step()\n",
        "          scheduler_ce.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      avg_loss = total_loss / len(DPRdataloader)\n",
        "      print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "  return question_encoder, context_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dH6e6Rq4a3wD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def saveChunkCounter(num_chunks, file_path='/content/drive/My Drive/data/number.txt'):\n",
        "    # Extract the directory path from the file_path\n",
        "    directory_path = os.path.dirname(file_path)\n",
        "\n",
        "    # Create the directory if it does not exist\n",
        "    os.makedirs(directory_path, exist_ok=True)\n",
        "\n",
        "    # Write num_chunks to the file\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(str(num_chunks))\n",
        "\n",
        "# Example usage\n",
        "# saveChunkCounter(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cAY6e9TZdfXB"
      },
      "outputs": [],
      "source": [
        "def readChunkCounter(file_path='/content/drive/My Drive/data/number.txt'):\n",
        "    # Ensure the file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        print(\"File does not exist.\")\n",
        "        return None\n",
        "\n",
        "    # Read the number from the file\n",
        "    with open(file_path, 'r') as file:\n",
        "        num_chunks = file.read()\n",
        "\n",
        "    # Convert the read string to an integer\n",
        "    try:\n",
        "        return int(num_chunks)\n",
        "    except ValueError:\n",
        "        print(\"Could not convert the file content to an integer.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "j1OlYDnrD9W8",
        "outputId": "119e17db-70a1-497c-8910-d783ccda98ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total entries: 82326\n",
            "Chunk size: 10\n",
            "Number of chunks: 176\n",
            "Number of chunks: 8057.6\n",
            "start_idx: 1760, end_idx: 1770\n",
            "current chunk 176\n",
            "84\n",
            "Epoch 1/1, Average Loss: 0.0025\n",
            "start_idx: 1770, end_idx: 1780\n",
            "current chunk 177\n",
            "89\n",
            "Epoch 1/1, Average Loss: 0.0026\n",
            "start_idx: 1780, end_idx: 1790\n",
            "current chunk 178\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-3b52cd74586a>\u001b[0m in \u001b[0;36mpreprocess_for_dpr_concurrently\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Wait for all futures to complete and collect their results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_futures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-3b1ac26b94f3>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Select the current chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcurrent_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mpreprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_for_dpr_concurrently\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_chunk\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mquestion_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_encoder_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_encoder_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-3b52cd74586a>\u001b[0m in \u001b[0;36mpreprocess_for_dpr_concurrently\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m\"\"\"Concurrent preprocessing for each item and its passages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdpr_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitem_executor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Submit all items for processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mitem_futures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_chunks_completed = int(readChunkCounter())\n",
        "total_entries = len(dataset['train'])\n",
        "chunk_size = 10\n",
        "num_chunks = (total_entries / chunk_size) + (1 if total_entries % chunk_size else 0) - num_chunks_completed\n",
        "print(f\"Total entries: {total_entries}\")\n",
        "print(f\"Chunk size: {chunk_size}\")\n",
        "print(f\"Number of chunks: {num_chunks_completed}\")\n",
        "print(f\"Number of chunks: {num_chunks}\")\n",
        "question_encoder, context_encoder = getEncoders()\n",
        "question_encoder_tokenizer, context_encoder_tokenizer = getTokenizers()\n",
        "\n",
        "for i in range(int(num_chunks)):\n",
        "    start_idx = num_chunks_completed * chunk_size\n",
        "    end_idx = (start_idx)  + (chunk_size)\n",
        "    # Use min to ensure end_idx does not exceed the total number of entries\n",
        "    end_idx = min(end_idx, total_entries)\n",
        "    print(f'start_idx: {start_idx}, end_idx: {end_idx}')\n",
        "    print('current chunk', num_chunks_completed)\n",
        "    # Select the current chunk\n",
        "    current_chunk = dataset['train'].select(range(start_idx, end_idx))\n",
        "    preprocessed_data = preprocess_for_dpr_concurrently({'train': current_chunk})\n",
        "    print(len(preprocessed_data))\n",
        "    question_encoder, context_encoder = trainModel(preprocessed_data, question_encoder, context_encoder, question_encoder_tokenizer, context_encoder_tokenizer)\n",
        "    saveModels(question_encoder, context_encoder)\n",
        "    num_chunks_completed += 1\n",
        "    saveChunkCounter(num_chunks_completed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HXb0pe6rRCxk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalize_embeddings(embeddings):\n",
        "    norms = embeddings.norm(p=2, dim=1, keepdim=True)\n",
        "    return embeddings.div(norms)\n",
        "\n",
        "def encode_query(query, question_encoder, tokenizer):\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(question_encoder.device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        query_embedding = question_encoder(**inputs).pooler_output\n",
        "    # Normalize the embedding\n",
        "    query_embedding = normalize_embeddings(query_embedding)\n",
        "    return query_embedding\n",
        "\n",
        "def encode_passages(passages, context_encoder, tokenizer):\n",
        "    inputs = tokenizer(passages, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(context_encoder.device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        passage_embeddings = context_encoder(**inputs).pooler_output\n",
        "    # Normalize the embeddings\n",
        "    passage_embeddings = normalize_embeddings(passage_embeddings)\n",
        "    return passage_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import nltk\n",
        "\n",
        "# Initialize NLP tools and models (assuming these functions are properly defined)\n",
        "device = torch.device(\"cpu\")\n",
        "question_encoder, context_encoder = getEncoders()\n",
        "question_encoder_tokenizer, context_encoder_tokenizer = getTokenizers()\n",
        "\n",
        "def preprocess_for_dpr_validation(validation_data):\n",
        "    preprocessed_data = []\n",
        "    for item in validation_data:\n",
        "        query = item['query']\n",
        "        passages = item['passages']['passage_text']\n",
        "        labels = item['passages']['is_selected']\n",
        "        preprocessed_item = {'query': query, 'passages': passages, 'labels': labels}\n",
        "        preprocessed_data.append(preprocessed_item)\n",
        "    return preprocessed_data\n",
        "\n",
        "def calculate_rank_for_item(item):\n",
        "    query, passages, labels = item['query'], item['passages'], item['labels']\n",
        "    question_input = question_encoder_tokenizer(query, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    question_embedding = question_encoder(**question_input).pooler_output\n",
        "    context_embeddings = []\n",
        "\n",
        "    for passage in passages:\n",
        "        context_input = context_encoder_tokenizer(passage, return_tensors='pt', padding=True, truncation=True, max_length=256).to(device)\n",
        "        context_embedding = context_encoder(**context_input).pooler_output\n",
        "        context_embeddings.append(context_embedding)\n",
        "\n",
        "    context_embeddings = torch.cat(context_embeddings, dim=0)\n",
        "    similarities = torch.matmul(question_embedding, context_embeddings.T).squeeze(0)\n",
        "    ranked_indices = torch.argsort(similarities, descending=True)\n",
        "    correct_indices = [i for i, label in enumerate(labels) if label == 1]\n",
        "\n",
        "    if correct_indices:\n",
        "        correct_index = correct_indices[0]\n",
        "        rank = (ranked_indices == correct_index).nonzero(as_tuple=True)[0].item() + 1\n",
        "        return 1.0 / rank\n",
        "    return 0\n",
        "\n",
        "def validate_and_calculate_mrr(preprocessed_data):\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        ranks = list(executor.map(calculate_rank_for_item, preprocessed_data))\n",
        "    mrr = np.mean([rank for rank in ranks if rank > 0]) if ranks else 0\n",
        "    return mrr\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "dataset = load_dataset(\"ms_marco\", \"v1.1\", split='validation')\n",
        "preprocessed_validation_data = preprocess_for_dpr_validation(dataset)\n",
        "\n",
        "# Calculate the MRR score\n",
        "mrr_score = validate_and_calculate_mrr(preprocessed_validation_data)\n",
        "print(f\"MRR Score: {mrr_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efFUPw_l2Qc7",
        "outputId": "51735a7e-5dfc-4433-d83f-a6da4772be8d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MRR Score: 0.4561778183352468\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}